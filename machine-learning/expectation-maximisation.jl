### A Pluto.jl notebook ###
# v0.12.17

using Markdown
using InteractiveUtils

# ‚ïî‚ïê‚ï° 102fce2e-13b9-11eb-0da5-ab502c3ea430
begin 
	import Pkg
	Pkg.activate("..")
	Pkg.instantiate()
	
	using LaTeXStrings,Parameters
	using LinearAlgebra
	using Zygote
	
	using WGLMakie,AbstractPlotting
	AbstractPlotting.inline!(true)
	using AbstractPlotting.MakieLayout
end

# ‚ïî‚ïê‚ï° 9114bea2-4373-11eb-3780-53292d278b2d
	md"""## Expectation Maximisation
	We shall demonstrate the expectation-maximisation algorithm on the gaussian
	mixture model ``p(\mathbf{x}|\mathbf{\theta})`` where parameters we want to 
	estimate ``\mathbf{\theta}=
	(\Sigma_1\dots,\Sigma_K,\mu_1\dots\mu_K,\pi_1\dots\pi_K)`` are the 
	means ``\mu_k`` covariance matrices ``\Sigma_k`` and weight ``\pi_k`` for each mixture. The model 
	is written as
	"""

# ‚ïî‚ïê‚ï° 9461a482-4060-11eb-2fd4-07e25338c86c
L"""
p(\mathbf{x}|\mathbf{\theta}):=
	\sum_{k=1}^K \pi_{k} \mathcal{N}(\mathbf{x}|\mu_k,\Sigma_k)
\quad\mathrm{where}\quad \sum_{k=1}^K\pi_k =1
"""

# ‚ïî‚ïê‚ï° 97e8d860-410e-11eb-2fd5-8de2aee692b0
function ùí©(x,Œº,Œ£) # multivariate gaussian
	return exp( -(x-Œº)'inv(Œ£)*(x-Œº)/2 ) / ‚àö( (2œÄ)^length(x)*abs(det(Œ£)) )
end

# ‚ïî‚ïê‚ï° a4935164-4062-11eb-3f47-0b92eaa966e9
md"""
We could just look for the maximum likelihood estimate. However this allows any optimiser to place mixture ``k`` right on top of a single data point and descrease the covariance ``\Sigma_k\rightarrow 0``. This leads to singularities in the likeliood and makes it very difficult to find the optimal parameters ``\theta^*``

Since optimising ``p(\mathbf{x}|\mathbf{\theta})`` directly is difficult we re-write the model in terms of ``K`` dimensional probability vectors ``\mathbf{z}``, its prior distirbution ``p(\mathbf{z}|\theta)`` and model ``p(\mathbf{x}|\mathbf{z},\theta)`` as
"""

# ‚ïî‚ïê‚ï° 0b784f32-408c-11eb-06c4-f50a21ab46cb
L"
p(\mathbf{x}|\theta)=\sum_{\mathbf{z}}p(\mathbf{x}|\mathbf{z},\theta)p(\mathbf{z}|\theta)
"

# ‚ïî‚ïê‚ï° 76e76e1e-4074-11eb-1a11-cfb9f827d4cd
L"""
p(\mathbf{x}|\mathbf{z},\mathbf{\theta}):=
	\prod_{k=1}^K \mathcal{N}(\mathbf{x}|\mu_k,\Sigma_k)^{z_k}
\qquad
p(\mathbf{z}|\theta):=\prod_{k=1}^K\pi_k^{z_k}
"""

# ‚ïî‚ïê‚ï° 074ede5a-4076-11eb-3a0a-955ff06976d9
function model(x::Vector{<:Number},z::Vector{<:Bool},Œ∏::NamedTuple)
	@assert(sum(z)==1,"z must be a one-hot vector")
	@unpack Œ£,Œº = Œ∏
	
	k = findfirst(z) # index of one-hot element
	return ùí©(x,Œº[k],Œ£[k])
end

# ‚ïî‚ïê‚ï° a39c0bdc-4073-11eb-086e-f592bb72706b
function prior(z::Vector{<:Bool},Œ∏::NamedTuple)
	@assert(sum(z)==1,"z must be a one-hot vector")
	@unpack œÄ = Œ∏

	k = findfirst(z) # index of one-hot element
	return œÄ[k]
end

# ‚ïî‚ïê‚ï° a091f3c8-408e-11eb-1d47-6b9b5044d660
md"""From the model and prior we can determine the joint distribution ``p(\mathbf{x},\mathbf{z}|\theta)`` and posterior ``p(\mathbf{z}|\mathbf{x},\theta)``
"""

# ‚ïî‚ïê‚ï° f1ec823e-4114-11eb-165b-55e6d09960e4
L"p(\mathbf{z}|\mathbf{x},\theta)=\frac{
p(\mathbf{x}|\mathbf{z},\theta)p(\mathbf{z}|\theta)}{
p(\mathbf{x}|\mathbf{\theta})}"

# ‚ïî‚ïê‚ï° 32c711cc-4078-11eb-2b47-8f9f2d6d256a
function joint(x::Vector{<:Number},z::Vector{<:Bool},Œ∏::NamedTuple)
	return model(x,z,Œ∏)*prior(z,Œ∏)
end

# ‚ïî‚ïê‚ï° 80e0b886-4078-11eb-2c90-ebfb374de47b
md"""Given a datasets ``\mathbf{X} = ( \mathbf{x}_1 \dots \mathbf{x}_D )`` and ``\mathbf{Z} = ( \mathbf{z}_1 \dots \mathbf{z}_D )`` we can calculate likelihoods for any distribution. This is because  the data are assumed independent and identically distributed, allowing the likelihoods factorise with respect to data. For example for the posterior likelihood
"""

# ‚ïî‚ïê‚ï° 997132d0-408a-11eb-22e0-976c2df8ca0c
L"p(\mathbf{Z}|\mathbf{X},\theta):=\prod_{\mathbf{x}\in\mathbf{X},\mathbf{z}\in\mathbf{Z}}p(\mathbf{z}|\mathbf{x},\theta)"

# ‚ïî‚ïê‚ï° d497c0f0-4115-11eb-1f01-df9a4d33dd08
L"=\mathrm{exp}\left( \sum_{\mathbf{x}\in\mathbf{X},\mathbf{z}\in\mathbf{Z}}\ln p(\mathbf{z}|\mathbf{x},\theta)\right)"

# ‚ïî‚ïê‚ï° 0f49f918-4090-11eb-3865-1771ff1ecb70
function joint(X::Vector{<:Vector},Z::Vector{<:Vector},Œ∏::NamedTuple)
	return exp( sum( log.( joint.(X,Z,Ref(Œ∏)) ) ) )
end

# ‚ïî‚ïê‚ï° c3b2b190-410f-11eb-1c38-ebbea30083d8
function marginal(x::Vector{<:Number},z::Vector{<:Bool},Œ∏::NamedTuple)
	return sum( k->joint(x, circshift(z,k) ,Œ∏), 1:length(z) ) # over all possible z
end

# ‚ïî‚ïê‚ï° 786906c2-4078-11eb-1a98-77134f10075d
function posterior(x::Vector{<:Number},z::Vector{<:Bool},Œ∏::NamedTuple)
	return joint(x,z,Œ∏) / marginal(x,z,Œ∏)
end

# ‚ïî‚ïê‚ï° b159cec8-408a-11eb-3728-0d266078dfa3
function posterior(X::Vector{<:Vector},Z::Vector{<:Vector},Œ∏::NamedTuple)
	return exp( sum( log.( posterior.(X,Z,Ref(Œ∏)) ) ) )
end

# ‚ïî‚ïê‚ï° ee8ca6b6-4121-11eb-2685-1bc42b156926
begin
	using CSV ##################################### data import
	data = CSV.File("data/old-faithful.tsv",type=Float64)
	data.waiting .= data.waiting/10.0
	
	xs = range(extrema(data.eruptions)..., length=100)
	ys = range(extrema(data.waiting)..., length=100)
	
	###################################################### scene construction
	scene, layout = layoutscene(resolution = (500,500))
	ax = layout[1,1] = LAxis(scene,
		
		xpanlock = true, xzoomlock = true,
		ypanlock = true, yzoomlock = true,
		
		xlabel="Eruptions", ylabel="Waiting")

	empty!(scene.events.mousedrag.listeners)
	mouseevents = addmouseevents!(ax.scene)
	
	######################################################### model parameters
	Œº‚ÇÅ,Œº‚ÇÇ= Node([4.0,5.0]), Node([2.5,8.0])
	Œ∏ = @lift((
		Œº=[ $Œº‚ÇÅ, $Œº‚ÇÇ ], œÄ=[ 1/2, 1/2 ],
		Œ£=[ [1/3 0; 0 1/3], [1/3 0; 0 1/3] ],
	))
	
	############################################################ marginal
	density = @lift([ marginal([x,y],[false,true],$Œ∏) for x in xs, y in ys])
	heatmap!( ax, xs, ys, density,

		colormap=cgrad(:starrynight, 10, categorical=true),
		interpolate=true )
	
	scatter!( ax, @lift(transpose($Œº‚ÇÅ)), marker="1", color=:white )
	scatter!( ax, @lift(transpose($Œº‚ÇÇ)), marker="2", color=:white )
	
	############################################################# posterior
	scatter!( ax, map( (x,y) -> Point(x,y), data.eruptions, data.waiting),
		markersize=@lift( map( (x,y) -> 5posterior([x,y],[false,true],$Œ∏), 
				data.eruptions, data.waiting )), color=:gold )
	
	scatter!( ax, map( (x,y) -> Point(x,y), data.eruptions, data.waiting),
		markersize=@lift( map( (x,y) -> 5posterior([x,y],[true,false],$Œ∏), 
				data.eruptions, data.waiting )), color=:white )
	
	###################################################### bind input events	
	on(scene.events.keyboardbuttons) do button
		
		if ispressed(button, Keyboard._1)
			Œº‚ÇÅ[] = mouseevents.obs.val.data
			
		elseif ispressed(button, Keyboard._2)
			Œº‚ÇÇ[] = mouseevents.obs.val.data
		end
	end
	
    RecordEvents( scene, "output" )
    scene
end

# ‚ïî‚ïê‚ï° 8a4a4e48-431c-11eb-07ff-09c8b60bd020
md"""
However in practice we are not given dataset ``\mathbf{Z}`` so the best thing we 
can do estimate what the probability of observing ``\mathbf{z}`` is given that we 
know ``\mathbf{x}``. This is precisely what we do in the expectation step, when 
we evaluate the posterior likelihood ``p(\mathbf{z}|\mathbf{x},\theta)``
"""

# ‚ïî‚ïê‚ï° 01ac872a-4113-11eb-1ffa-f3087641bcf6
md"""
Let's look at this posterior for the old faithful dataset which has dimensions 
``N=2`` and seems to have ``K=2`` mixtures. We shall initialise a random 
``\theta`` and plot the contours of gaussian mixtures ``p(\mathbf{x}|\theta)``. 
**You can change the locations of the two mixtures my clicking on the plot, then 
press 1 or 2 on your keyboard when you would like to place the first or second 
mixture.**
"""

# ‚ïî‚ïê‚ï° 2d939648-431c-11eb-16c4-136bd9e12d8e
md"""
We shall visualise the likelihood of each data point ``p(\mathbf{z}|\mathbf{x},\theta)`` as a colour: white if the probability of the first mixture ``\mathbf{z}=(1,0)`` is greater than the probability of the second ``\mathbf{z}=(0,1)`` and gold otherwise.
"""

# ‚ïî‚ïê‚ï° cbc6114a-4314-11eb-1593-f588436f287c
md"""
We can see that the closer a data point ``\mathbf{x}`` is to mixture ``k``, the higher the probability ``p(\mathbf{z}|\mathbf{x},\theta)`` such that ``\mathbf{z}`` is a one-hot vector where the ``k``-th component is non-zero. How do we design an iterative proceedure to bring the mixtures to overlap with the high data density regions?

The evaluation of posterior likelihood ``p(\mathbf{z}|\mathbf{x},\theta)`` is known as the **expectation** step. What mixture do we expect each data point to belong to? In general, what do we expect the values of our latent variables ``\mathbf{z}`` to be given that we have ``\mathbf{x}``?

The **maximisation** step involves maximising the joint log likelihood ``\ln p(\mathbf{x},\mathbf{z}|\theta')`` as we would in the usual maximum likelihood approach, but here we take into account the expected values ``\mathbf{z}``. This can be done by averaging over ``\mathbf{Z}`` and since we have ``p(\mathbf{z}|\mathbf{x},\theta)`` we can write this as
"""

# ‚ïî‚ïê‚ï° 95544938-4319-11eb-339b-e9349790aea3
L"
Q(\theta,\theta'):=\sum_{\mathbf{z}} p(\mathbf{z}|\mathbf{x},\theta) \ln p(\mathbf{x},\mathbf{z}|\theta')
"

# ‚ïî‚ïê‚ï° 9009b516-431a-11eb-17ce-998e61d11549
function Q(Œ∏::NamedTuple,Œ∏‚Ä≤::NamedTuple,x::Vector{<:Number})
	Z = [[true,false],[false,true]] # specific for two mixtures
	return sum( z->posterior(x,z,Œ∏)*log(joint(x,z,Œ∏‚Ä≤)), Z )
end

# ‚ïî‚ïê‚ï° aa8e98e2-4320-11eb-267d-ddd980a07d3f
md"
Since likelihoods factorise with respect whole datasets ``\mathbf{X}``, leading to sums in log likelihoods, the function ``Q(\theta,\theta')`` can be evaluated as a sum over the whole dataset
"

# ‚ïî‚ïê‚ï° efb77868-431f-11eb-2173-8bb0a2cf97c9
function Q(Œ∏::NamedTuple,Œ∏‚Ä≤::NamedTuple,X::Vector{<:Vector})
	return sum( x->Q(Œ∏,Œ∏‚Ä≤,x), X )
end

# ‚ïî‚ïê‚ï° e33547b0-4319-11eb-1f54-ff0d5101b85c
md"
We would maximise this function with respect to an unknown ``\theta'`` keeping the initial ``\theta`` we used for the expectation step fixed. Once the maximum has been found for some optimal ``\theta'`` we can update the parameter ``\theta\leftarrow\theta'`` and repeat the proceedure.
"

# ‚ïî‚ïê‚ï° db2cd3b0-431b-11eb-0635-0d71b8e3ab29
begin
	
	############################# initialise dataset
	X = map( (x,y)->[x,y], data.eruptions, data.waiting )
	
	############################# initial parameters
	Œ∏‚Çú = ( Œº=[ [4.0,5.0], [2.5,8.0] ], œÄ=[ 1/2, 1/2 ],
		   Œ£=[ [1/3 0; 0 1/3], [1/3 0; 0 1/3] ] )
	
	Œ∑ = 10^-3 ###### learning rate
	for _ ‚àà 1:40 ### algorithm iterations

		######################## expectaction-maximisation
		‚àÇŒ∏, = gradient( Œ∏ -> Q(Œ∏‚Çú,Œ∏,X), Œ∏‚Çú )

		###################### update parameters
		Œ∏‚Çú.Œº .= Œ∏‚Çú.Œº + Œ∑*‚àÇŒ∏.Œº
		Œ∏‚Çú.Œ£ .= Œ∏‚Çú.Œ£ + Œ∑*‚àÇŒ∏.Œ£
		Œ∏‚Çú.œÄ .= Œ∏‚Çú.œÄ + Œ∑*‚àÇŒ∏.œÄ

		###################### enforce constraints
		Œ∏‚Çú.œÄ ./= sum(Œ∏‚Çú.œÄ)
	end
end

# ‚ïî‚ïê‚ï° 49de5dc2-4328-11eb-3352-57e88bddb934
begin
	###################################################### scene construction
	scene‚ÇÅ, layout‚ÇÅ = layoutscene(resolution = (500,500))
	ax‚ÇÅ = layout‚ÇÅ[1,1] = LAxis(scene‚ÇÅ,
		
		xpanlock = true, xzoomlock = true,
		ypanlock = true, yzoomlock = true,
		
		xlabel="Eruptions", ylabel="Waiting")
	empty!(scene‚ÇÅ.events.mousedrag.listeners)
	
	############################################################ marginal
	heatmap!( ax‚ÇÅ, xs, ys, [ marginal([x,y],[false,true],Œ∏‚Çú) for x in xs, y in ys],
		colormap=cgrad(:starrynight, 10, categorical=true),
		interpolate=true )
	
	############################################################# posterior
	scatter!( ax‚ÇÅ, map( (x,y) -> Point(x,y), data.eruptions, data.waiting),
		markersize= map( (x,y) -> 5posterior([x,y],[false,true],Œ∏‚Çú), 
				data.eruptions, data.waiting ), color=:gold )
	
	scatter!( ax‚ÇÅ, map( (x,y) -> Point(x,y), data.eruptions, data.waiting),
		markersize= map( (x,y) -> 5posterior([x,y],[true,false],Œ∏‚Çú), 
				data.eruptions, data.waiting ), color=:white )
	
    RecordEvents( scene‚ÇÅ, "output" )
    scene‚ÇÅ
end

# ‚ïî‚ïê‚ï° e88e67b6-4317-11eb-1a9f-3b7864692294
md"""
In fact ``K``-means would suffice for this task so why are we bothing with this more complicated approach? Rather than assigning each data point to a cluster centroid like in  ``K``-means, each data point has a probability of belonging to a mixture ``p(\mathbf{z}|\mathbf{x},\theta)`` . 

Furthermore we are not limited the gaussian mixtures and can consider more complicated distributions ``p(\mathbf{x}|\mathbf{z},\theta)``. Finally we can consider the problem of latent variables in general, where ``\mathbf{z}`` is not just a one-hot encoding of a mixture.
"""

# ‚ïî‚ïê‚ï° Cell order:
# ‚ïü‚îÄ102fce2e-13b9-11eb-0da5-ab502c3ea430
# ‚ïü‚îÄ9114bea2-4373-11eb-3780-53292d278b2d
# ‚ïü‚îÄ9461a482-4060-11eb-2fd4-07e25338c86c
# ‚ï†‚ïê97e8d860-410e-11eb-2fd5-8de2aee692b0
# ‚ïü‚îÄa4935164-4062-11eb-3f47-0b92eaa966e9
# ‚ïü‚îÄ0b784f32-408c-11eb-06c4-f50a21ab46cb
# ‚ïü‚îÄ76e76e1e-4074-11eb-1a11-cfb9f827d4cd
# ‚ï†‚ïê074ede5a-4076-11eb-3a0a-955ff06976d9
# ‚ï†‚ïêa39c0bdc-4073-11eb-086e-f592bb72706b
# ‚ïü‚îÄa091f3c8-408e-11eb-1d47-6b9b5044d660
# ‚ïü‚îÄf1ec823e-4114-11eb-165b-55e6d09960e4
# ‚ï†‚ïê32c711cc-4078-11eb-2b47-8f9f2d6d256a
# ‚ï†‚ïêc3b2b190-410f-11eb-1c38-ebbea30083d8
# ‚ï†‚ïê786906c2-4078-11eb-1a98-77134f10075d
# ‚ïü‚îÄ80e0b886-4078-11eb-2c90-ebfb374de47b
# ‚ïü‚îÄ997132d0-408a-11eb-22e0-976c2df8ca0c
# ‚ïü‚îÄd497c0f0-4115-11eb-1f01-df9a4d33dd08
# ‚ï†‚ïê0f49f918-4090-11eb-3865-1771ff1ecb70
# ‚ï†‚ïêb159cec8-408a-11eb-3728-0d266078dfa3
# ‚ïü‚îÄ8a4a4e48-431c-11eb-07ff-09c8b60bd020
# ‚ïü‚îÄ01ac872a-4113-11eb-1ffa-f3087641bcf6
# ‚ïü‚îÄ2d939648-431c-11eb-16c4-136bd9e12d8e
# ‚ïü‚îÄee8ca6b6-4121-11eb-2685-1bc42b156926
# ‚ïü‚îÄcbc6114a-4314-11eb-1593-f588436f287c
# ‚ïü‚îÄ95544938-4319-11eb-339b-e9349790aea3
# ‚ï†‚ïê9009b516-431a-11eb-17ce-998e61d11549
# ‚ïü‚îÄaa8e98e2-4320-11eb-267d-ddd980a07d3f
# ‚ï†‚ïêefb77868-431f-11eb-2173-8bb0a2cf97c9
# ‚ïü‚îÄe33547b0-4319-11eb-1f54-ff0d5101b85c
# ‚ï†‚ïêdb2cd3b0-431b-11eb-0635-0d71b8e3ab29
# ‚ïü‚îÄ49de5dc2-4328-11eb-3352-57e88bddb934
# ‚ïü‚îÄe88e67b6-4317-11eb-1a9f-3b7864692294
